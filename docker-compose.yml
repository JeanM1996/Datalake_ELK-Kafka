version: "3.8"

services:
  zookeeper:
    image: "docker.io/bitnami/zookeeper:3-debian-10"
    container_name: zookeeper
    hostname: zookeeper
    ports:
      - "2181:2181"
    volumes:
      - "zookeeper_data:/smart_phone_iot"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    networks:
      - streaming
  kafka:
    image: "docker.io/bitnami/kafka:2-debian-10"
    container_name: kafka
    hostname: kafka
    ports:
      - "9092:9092"
      - "9093:9093"
    volumes:
      - "kafka_data:/smart_phone_iot"
    environment:
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=CLIENT://:9092,EXTERNAL://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093
      - KAFKA_INTER_BROKER_LISTENER_NAME=CLIENT
    networks:
      - streaming
    depends_on:
      - zookeeper
    links:
      - zookeeper
  producer:
    build:
      context: ./kafka-producer
    container_name: kafka-producer
    restart: always
    ports:
      - "5555:5555/udp"
    networks:
      - streaming
    depends_on:
      - kafka
    links:
      - kafka

  elasticsearch:
    image: elasticsearch:7.7.0
    container_name: elasticsearch1
    hostname: elasticsearch
    environment:
      - "discovery.type=single-node"
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - streaming
  kibana:
    image: kibana:7.7.0
    container_name: kibana1
    hostname: kibana
    ports:
      - "5601:5601"
    links:
      - elasticsearch:elasticsearch1
    depends_on:
      - elasticsearch
    networks:
      - streaming
  logstash:
    image: logstash:7.7.0
    container_name: logstash1
    hostname: logstash
    ports:
      - "9600:9600"
      - "8089:8089"
    volumes:
      - ./logstash:/usr/share/logstash/pipeline/
    links:
      - elasticsearch:elasticsearch1
      - kafka
    depends_on:
      - elasticsearch
      - kafka
    networks:
      - streaming
  
  logstashB:
    image: logstash:7.7.0
    container_name: logstashB
    hostname: logstashB
    ports:
      - 5044:5044
    volumes:
      - ./filebeat/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - ./filebeat/logstash.template.json:/usr/share/logstash/templates/logstash.template.json
    links:
      - elasticsearch:elasticsearch1
    depends_on:
      - elasticsearch
    networks:
      - streaming

  filebeat:
    user: root
    container_name: filebeat
    image: docker.elastic.co/beats/filebeat:7.7.0
    links:
      - logstashB:logstashB
    depends_on:
      - logstashB
    volumes:
      - /var/run/docker.sock:/host_docker/docker.sock
      - /var/lib/docker:/host_docker/var/lib/docker
      - ./filebeat/localdata:/usr/share/filebeat/localdata
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml
    command: ["--strict.perms=false"]
    ulimits:
      memlock:
        soft: -1
        hard: -1
    stdin_open: true
    tty: true
    networks:
      - streaming
    deploy:
      mode: global
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "50"

volumes:
  zookeeper_data:
    driver: local
  kafka_data:
    driver: local

networks:
  streaming:
    driver: bridge
